{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowers Recognition\n",
    "### By : Muhammad Alfarizi Tazkia\n",
    "### Dataset Link : https://www.kaggle.com/alxmamaev/flowers-recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'archive-dataset', 'flowers-recognition.ipynb', '.git']\n"
     ]
    }
   ],
   "source": [
    "# show directory\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archive(3).zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('archive-dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracting dataset!\n"
     ]
    }
   ],
   "source": [
    "dataset_zip = 'archive-dataset/archive(3).zip'\n",
    "os.mkdir('dataset')\n",
    "dataset_dir = 'dataset/'\n",
    "\n",
    "scanning_zip_file = zipfile.ZipFile(dataset_zip, 'r')\n",
    "scanning_zip_file.extractall(dataset_dir)\n",
    "scanning_zip_file.close()\n",
    "\n",
    "print('Successfully extracting dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', '.ipynb_checkpoints', 'archive-dataset', 'flowers-recognition.ipynb', '.git']\n",
      "['rose', 'tulip', 'daisy', 'dandelion', 'sunflower']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())\n",
    "print(os.listdir(os.path.join(dataset_dir, 'flowers')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Directory Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train']\n"
     ]
    }
   ],
   "source": [
    "os.rename(os.path.join(dataset_dir, 'flowers'), os.path.join(dataset_dir, 'train'))\n",
    "print(os.listdir(dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(dataset_dir, 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'val']\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "val_dir = os.path.join(dataset_dir, 'val')\n",
    "print(os.listdir(dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_portion = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rose before splitting : 784 files\n",
      "rose after splitting : 627 files\n",
      "\n",
      "tulip before splitting : 984 files\n",
      "tulip after splitting : 787 files\n",
      "\n",
      "daisy before splitting : 764 files\n",
      "daisy after splitting : 611 files\n",
      "\n",
      "dandelion before splitting : 1052 files\n",
      "dandelion after splitting : 841 files\n",
      "\n",
      "sunflower before splitting : 733 files\n",
      "sunflower after splitting : 586 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category in os.listdir(train_dir):\n",
    "    current_dir = os.path.join(train_dir, category)\n",
    "    dir_length = len(os.listdir(current_dir))\n",
    "    counter = 0\n",
    "    max_training = split_portion * dir_length\n",
    "    \n",
    "    os.mkdir(os.path.join(val_dir, category))\n",
    "    target_dir = os.path.join(val_dir, category)\n",
    "    \n",
    "    print(\"{} before splitting : {} files\".format(category, dir_length))\n",
    "    \n",
    "    for file in os.listdir(current_dir):\n",
    "        src = os.path.join(current_dir, file)\n",
    "        target = os.path.join(target_dir, file)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if counter >= max_training:\n",
    "            shutil.move(src, target)\n",
    "            \n",
    "    print(\"{} after splitting : {} files\".format(category, len(os.listdir(current_dir))))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rule = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    shear_range = 0.2,\n",
    "    rotation_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    fill_mode = 'nearest'\n",
    ")\n",
    "\n",
    "val_rule = ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3452 images belonging to 5 classes.\n",
      "Found 865 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_rule.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 20,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_generator = val_rule.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 20,\n",
    "    class_mode = 'categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_106 (Conv2D)          (None, 148, 148, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_106 (MaxPoolin (None, 74, 74, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, 72, 72, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_107 (MaxPoolin (None, 36, 36, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 128)               2654336   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,690,037\n",
      "Trainable params: 2,690,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "173/173 [==============================] - 64s 365ms/step - loss: 1.4072 - acc: 0.3789 - val_loss: 1.2686 - val_acc: 0.4786\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 65s 371ms/step - loss: 1.2281 - acc: 0.4829 - val_loss: 1.1623 - val_acc: 0.5410\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 69s 399ms/step - loss: 1.1289 - acc: 0.5298 - val_loss: 1.0920 - val_acc: 0.5653\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 64s 370ms/step - loss: 1.1038 - acc: 0.5449 - val_loss: 1.0174 - val_acc: 0.5965\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 60s 344ms/step - loss: 1.0753 - acc: 0.5663 - val_loss: 1.1873 - val_acc: 0.5387\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 60s 347ms/step - loss: 1.0461 - acc: 0.5811 - val_loss: 1.0027 - val_acc: 0.6451\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 60s 345ms/step - loss: 1.0076 - acc: 0.5985 - val_loss: 0.9398 - val_acc: 0.6347\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 59s 341ms/step - loss: 0.9900 - acc: 0.5936 - val_loss: 0.9809 - val_acc: 0.6428\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 60s 345ms/step - loss: 0.9594 - acc: 0.6338 - val_loss: 0.9553 - val_acc: 0.6439\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 59s 341ms/step - loss: 0.9490 - acc: 0.6260 - val_loss: 0.9170 - val_acc: 0.6671\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(8, (3, 3), activation = 'relu', input_shape = (150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(16, (3, 3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    Dense(5, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs = 10,\n",
    "    validation_data = val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
